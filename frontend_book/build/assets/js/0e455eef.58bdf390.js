"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[4012],{1936(n,i,e){e.r(i),e.d(i,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"docs/digital-twin/unity-environments","title":"High-Fidelity Environments with Unity","description":"Creating realistic visual environments for robotics simulation with Unity","source":"@site/docs/docs/digital-twin/unity-environments.md","sourceDirName":"docs/digital-twin","slug":"/docs/digital-twin/unity-environments","permalink":"/docs/docs/digital-twin/unity-environments","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docs/digital-twin/unity-environments.md","tags":[],"version":"current","frontMatter":{"title":"High-Fidelity Environments with Unity","sidebar_label":"Unity Environments","description":"Creating realistic visual environments for robotics simulation with Unity","keywords":["Unity","Visual Simulation","Digital Twin","Human-Robot Interaction","ROS 2"]},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation","permalink":"/docs/docs/digital-twin/physics-simulation"},"next":{"title":"Sensor Simulation","permalink":"/docs/docs/digital-twin/sensor-simulation"}}');var s=e(4848),o=e(8453);const r={title:"High-Fidelity Environments with Unity",sidebar_label:"Unity Environments",description:"Creating realistic visual environments for robotics simulation with Unity",keywords:["Unity","Visual Simulation","Digital Twin","Human-Robot Interaction","ROS 2"]},a="High-Fidelity Environments with Unity",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Purpose of Unity in Robotics Simulation",id:"purpose-of-unity-in-robotics-simulation",level:2},{value:"Visual Realism",id:"visual-realism",level:3},{value:"Human-Robot Interaction Studies",id:"human-robot-interaction-studies",level:3},{value:"Perception Testing",id:"perception-testing",level:3},{value:"Unity-ROS 2 Communication Concepts",id:"unity-ros-2-communication-concepts",level:2},{value:"The Unity Robotics Hub",id:"the-unity-robotics-hub",level:3},{value:"Communication Architecture",id:"communication-architecture",level:3},{value:"Message Mapping",id:"message-mapping",level:3},{value:"Service and Action Integration",id:"service-and-action-integration",level:3},{value:"Creating Realistic Environments",id:"creating-realistic-environments",level:2},{value:"Environment Design Principles",id:"environment-design-principles",level:3},{value:"Realism vs. Performance Trade-offs",id:"realism-vs-performance-trade-offs",level:4},{value:"Physical Accuracy",id:"physical-accuracy",level:4},{value:"Sample Environment Setup",id:"sample-environment-setup",level:3},{value:"Human-Robot Interaction in Unity",id:"human-robot-interaction-in-unity",level:2},{value:"Interaction Design Principles",id:"interaction-design-principles",level:3},{value:"Intuitive Controls",id:"intuitive-controls",level:4},{value:"Feedback Mechanisms",id:"feedback-mechanisms",level:4},{value:"VR Integration for Immersive Interaction",id:"vr-integration-for-immersive-interaction",level:3},{value:"Unity-ROS 2 Bridge Implementation",id:"unity-ros-2-bridge-implementation",level:2},{value:"Network Configuration",id:"network-configuration",level:3},{value:"Best Practices for Unity Robotics Simulation",id:"best-practices-for-unity-robotics-simulation",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Quality Assurance",id:"quality-assurance",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"high-fidelity-environments-with-unity",children:"High-Fidelity Environments with Unity"})}),"\n",(0,s.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(i.p,{children:"Unity provides the visual layer for digital twin applications in robotics, offering high-fidelity rendering capabilities that create realistic environments for human-robot interaction studies. Unlike Gazebo which focuses on physics simulation, Unity specializes in visual realism and immersive experiences that are crucial for perception testing and human-robot interaction research."}),"\n",(0,s.jsx)(i.h2,{id:"purpose-of-unity-in-robotics-simulation",children:"Purpose of Unity in Robotics Simulation"}),"\n",(0,s.jsx)(i.p,{children:"Unity serves several critical functions in the robotics simulation pipeline:"}),"\n",(0,s.jsx)(i.h3,{id:"visual-realism",children:"Visual Realism"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Photorealistic Rendering"}),": Advanced lighting, materials, and post-processing effects"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"High-Quality Graphics"}),": Detailed textures, realistic lighting models, and complex shaders"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Environmental Effects"}),": Weather systems, dynamic lighting, and atmospheric conditions"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Realistic Materials"}),": Physically Based Rendering (PBR) materials that behave like real surfaces"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"human-robot-interaction-studies",children:"Human-Robot Interaction Studies"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Immersive Visualization"}),": 3D environments that allow researchers to observe robot behavior"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Virtual Reality Support"}),": VR headsets for first-person robot operation experiences"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"User Interface Prototyping"}),": Testing human-robot interfaces in safe virtual environments"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Behavior Analysis"}),": Studying how humans interact with robots in realistic settings"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"perception-testing",children:"Perception Testing"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Synthetic Data Generation"}),": Creating training data for computer vision algorithms"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Sensor Simulation"}),": Visual sensors like cameras in realistic environments"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Lighting Condition Testing"}),": Various lighting scenarios without physical setup"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Dataset Creation"}),": Large-scale synthetic datasets for AI training"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"unity-ros-2-communication-concepts",children:"Unity-ROS 2 Communication Concepts"}),"\n",(0,s.jsx)(i.h3,{id:"the-unity-robotics-hub",children:"The Unity Robotics Hub"}),"\n",(0,s.jsx)(i.p,{children:"The Unity Robotics Hub provides essential tools for connecting Unity with ROS 2:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"ROS-TCP-Connector"}),": Network bridge for communication between Unity and ROS 2"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"URDF Importer"}),": Tool for importing robot models from ROS URDF files"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"ML-Agents"}),": Machine learning framework for training robot behaviors"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Robotics Examples"}),": Sample projects demonstrating best practices"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"communication-architecture",children:"Communication Architecture"}),"\n",(0,s.jsx)(i.p,{children:"The communication between Unity and ROS 2 follows a client-server model:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:"ROS 2 System \u2190\u2192 TCP/IP Network \u2190\u2192 Unity Application\n"})}),"\n",(0,s.jsx)(i.p,{children:"Unity acts as a client that connects to the ROS 2 network through the TCP connector, enabling bidirectional communication."}),"\n",(0,s.jsx)(i.h3,{id:"message-mapping",children:"Message Mapping"}),"\n",(0,s.jsx)(i.p,{children:"Unity uses custom data structures that map to ROS 2 message types:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-csharp",children:'using ROS2;\nusing Unity.Robotics.ROSTCPConnector;\nusing sensor_msgs.msg;\n\npublic class UnitySensorPublisher : MonoBehaviour\n{\n    ROSConnection ros;\n    public string topicName = "/unity_camera_image";\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n        ros.RegisterPublisher<CompressedImageMsg>(topicName);\n    }\n\n    void PublishImage(Texture2D image)\n    {\n        byte[] imageBytes = image.EncodeToJPG();\n        CompressedImageMsg msg = new CompressedImageMsg\n        {\n            format = "jpeg",\n            data = imageBytes\n        };\n        ros.Publish(topicName, msg);\n    }\n}\n'})}),"\n",(0,s.jsx)(i.h3,{id:"service-and-action-integration",children:"Service and Action Integration"}),"\n",(0,s.jsx)(i.p,{children:"Unity can also call ROS 2 services and participate in action servers:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Services"}),": Synchronous request-response communication"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Actions"}),": Asynchronous goal-oriented communication with feedback"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Topics"}),": Publish-subscribe communication pattern"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"creating-realistic-environments",children:"Creating Realistic Environments"}),"\n",(0,s.jsx)(i.h3,{id:"environment-design-principles",children:"Environment Design Principles"}),"\n",(0,s.jsx)(i.p,{children:"Effective robotics simulation environments should include:"}),"\n",(0,s.jsx)(i.h4,{id:"realism-vs-performance-trade-offs",children:"Realism vs. Performance Trade-offs"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Level of Detail (LOD)"}),": Automatically adjust model complexity based on distance"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Occlusion Culling"}),": Don't render objects not visible to cameras"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Texture Streaming"}),": Load textures on-demand based on visibility"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Dynamic Batching"}),": Combine similar objects for efficient rendering"]}),"\n"]}),"\n",(0,s.jsx)(i.h4,{id:"physical-accuracy",children:"Physical Accuracy"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Scale Accuracy"}),": Maintain real-world proportions for accurate sensor simulation"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Material Properties"}),": Match real-world reflectance and surface characteristics"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Lighting Conditions"}),": Replicate real-world lighting scenarios"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Environmental Physics"}),": Include wind, gravity, and other environmental forces"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"sample-environment-setup",children:"Sample Environment Setup"}),"\n",(0,s.jsx)(i.p,{children:"Creating a basic indoor robotics environment in Unity:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class IndoorEnvironment : MonoBehaviour\n{\n    [Header("Environment Settings")]\n    public float roomWidth = 10f;\n    public float roomLength = 15f;\n    public float roomHeight = 3f;\n\n    [Header("Lighting")]\n    public Light mainLight;\n    public float lightIntensity = 1.0f;\n\n    void Start()\n    {\n        CreateEnvironment();\n        SetupLighting();\n    }\n\n    void CreateEnvironment()\n    {\n        // Create floor\n        GameObject floor = GameObject.CreatePrimitive(PrimitiveType.Cube);\n        floor.transform.localScale = new Vector3(roomWidth, 0.1f, roomLength);\n        floor.transform.position = new Vector3(0, -0.05f, 0);\n        floor.name = "Floor";\n\n        // Create walls\n        CreateWalls();\n    }\n\n    void CreateWalls()\n    {\n        // Create four walls\n        CreateWall(new Vector3(0, roomHeight/2, roomLength/2),\n                  new Vector3(roomWidth, roomHeight, 0.1f));\n        CreateWall(new Vector3(0, roomHeight/2, -roomLength/2),\n                  new Vector3(roomWidth, roomHeight, 0.1f));\n        CreateWall(new Vector3(roomWidth/2, roomHeight/2, 0),\n                  new Vector3(0.1f, roomHeight, roomLength));\n        CreateWall(new Vector3(-roomWidth/2, roomHeight/2, 0),\n                  new Vector3(0.1f, roomHeight, roomLength));\n    }\n\n    GameObject CreateWall(Vector3 position, Vector3 scale)\n    {\n        GameObject wall = GameObject.CreatePrimitive(PrimitiveType.Cube);\n        wall.transform.position = position;\n        wall.transform.localScale = scale;\n        wall.GetComponent<Renderer>().material.color = Color.gray;\n        return wall;\n    }\n\n    void SetupLighting()\n    {\n        mainLight.intensity = lightIntensity;\n        mainLight.type = LightType.Directional;\n    }\n}\n'})}),"\n",(0,s.jsx)(i.h2,{id:"human-robot-interaction-in-unity",children:"Human-Robot Interaction in Unity"}),"\n",(0,s.jsx)(i.h3,{id:"interaction-design-principles",children:"Interaction Design Principles"}),"\n",(0,s.jsx)(i.p,{children:"Creating effective human-robot interaction experiences requires:"}),"\n",(0,s.jsx)(i.h4,{id:"intuitive-controls",children:"Intuitive Controls"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Natural Movement"}),": Allow humans to move through the environment naturally"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Robot Control Interfaces"}),": Provide multiple ways to control robots"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Gesture Recognition"}),": Simulate gesture-based interaction"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Voice Commands"}),": Integrate speech recognition for voice control"]}),"\n"]}),"\n",(0,s.jsx)(i.h4,{id:"feedback-mechanisms",children:"Feedback Mechanisms"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Visual Feedback"}),": Clear indicators of robot state and intentions"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Audio Feedback"}),": Sound effects and speech for communication"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Haptic Feedback"}),": Simulated touch feedback in VR environments"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Status Indicators"}),": Clear visualization of robot capabilities and status"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"vr-integration-for-immersive-interaction",children:"VR Integration for Immersive Interaction"}),"\n",(0,s.jsx)(i.p,{children:"Unity's VR capabilities enhance human-robot interaction studies:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-csharp",children:'using UnityEngine.XR;\nusing UnityEngine.XR.Interaction.Toolkit;\n\npublic class VRRobotController : MonoBehaviour\n{\n    [Header("VR Interaction")]\n    public XRRayInteractor leftRay;\n    public XRRayInteractor rightRay;\n    public XRGrabInteractable robotController;\n\n    void Update()\n    {\n        HandleVRInteraction();\n    }\n\n    void HandleVRInteraction()\n    {\n        // Check for interaction with robot controls\n        if (robotController.isSelected)\n        {\n            // Process robot control commands\n            ProcessRobotCommands();\n        }\n    }\n\n    void ProcessRobotCommands()\n    {\n        // Map VR controller inputs to robot commands\n        // This could include movement, manipulation, or other robot functions\n    }\n}\n'})}),"\n",(0,s.jsx)(i.h2,{id:"unity-ros-2-bridge-implementation",children:"Unity-ROS 2 Bridge Implementation"}),"\n",(0,s.jsx)(i.h3,{id:"network-configuration",children:"Network Configuration"}),"\n",(0,s.jsx)(i.p,{children:"Setting up the TCP bridge between Unity and ROS 2:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-csharp",children:'using Unity.Robotics.ROSTCPConnector;\n\npublic class UnityROSBridge : MonoBehaviour\n{\n    [Header("ROS Connection Settings")]\n    public string rosIP = "127.0.0.1";\n    public int rosPort = 10000;\n\n    void Start()\n    {\n        // Configure ROS connection\n        ROSConnection.instance.SetHostname(rosIP, rosPort);\n\n        // Initialize publishers and subscribers\n        InitializeROSCommunication();\n    }\n\n    void InitializeROSCommunication()\n    {\n        // Register publishers\n        ROSConnection.instance.RegisterPublisher<geometry_msgs.msg.Twist>("/cmd_vel");\n        ROSConnection.instance.RegisterPublisher<sensor_msgs.msg.JointState>("/joint_states");\n\n        // Register subscribers\n        ROSConnection.instance.Subscribe<sensor_msgs.msg.LaserScan>("/scan", OnLaserScan);\n        ROSConnection.instance.Subscribe<nav_msgs.msg.Odometry>("/odom", OnOdometry);\n    }\n\n    void OnLaserScan(sensor_msgs.msg.LaserScan scanData)\n    {\n        // Process laser scan data from ROS\n        // Update Unity visualization based on sensor data\n    }\n\n    void OnOdometry(nav_msgs.msg.Odometry odomData)\n    {\n        // Update robot position in Unity based on ROS odometry\n        UpdateRobotPosition(odomData.pose.pose);\n    }\n\n    void UpdateRobotPosition(geometry_msgs.msg.Pose pose)\n    {\n        // Apply pose to robot GameObject in Unity\n        transform.position = new Vector3(pose.position.x, pose.position.y, pose.position.z);\n        transform.rotation = new Quaternion(pose.orientation.x, pose.orientation.y,\n                                          pose.orientation.z, pose.orientation.w);\n    }\n}\n'})}),"\n",(0,s.jsx)(i.h2,{id:"best-practices-for-unity-robotics-simulation",children:"Best Practices for Unity Robotics Simulation"}),"\n",(0,s.jsx)(i.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"LOD Systems"}),": Use Level of Detail to reduce complexity at distance"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Occlusion Culling"}),": Don't render objects blocked by other objects"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Texture Atlasing"}),": Combine multiple textures into single atlases"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Object Pooling"}),": Reuse objects instead of creating/destroying frequently"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Consistent Scale"}),": Maintain real-world scale across all assets"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Physics Accuracy"}),": Ensure visual simulation matches physical simulation"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Sensor Accuracy"}),": Validate that simulated sensors match real sensor characteristics"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Timing Consistency"}),": Maintain synchronization between Unity and ROS 2 clocks"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(i.p,{children:"Unity provides the high-fidelity visual layer essential for digital twin applications in robotics. Its ability to create realistic environments and support human-robot interaction studies makes it an invaluable tool for robotics research. The integration with ROS 2 through the Unity Robotics Hub enables seamless data exchange between the visual simulation and robotic control systems."}),"\n",(0,s.jsx)(i.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Unity specializes in visual realism and human-robot interaction"}),"\n",(0,s.jsx)(i.li,{children:"The Unity-ROS 2 bridge enables bidirectional communication"}),"\n",(0,s.jsx)(i.li,{children:"High-fidelity environments are crucial for perception testing"}),"\n",(0,s.jsx)(i.li,{children:"VR capabilities enhance human-robot interaction studies"}),"\n",(0,s.jsx)(i.li,{children:"Performance optimization is essential for real-time simulation"}),"\n"]})]})}function h(n={}){const{wrapper:i}={...(0,o.R)(),...n.components};return i?(0,s.jsx)(i,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,i,e){e.d(i,{R:()=>r,x:()=>a});var t=e(6540);const s={},o=t.createContext(s);function r(n){const i=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function a(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),t.createElement(o.Provider,{value:i},n.children)}}}]);