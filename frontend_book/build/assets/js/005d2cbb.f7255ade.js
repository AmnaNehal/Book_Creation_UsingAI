"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[105],{1647(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"docs/digital-twin/sensor-simulation","title":"Sensor Simulation for Humanoids","description":"Simulating various sensors for humanoid robots in digital twin environments","source":"@site/docs/docs/digital-twin/sensor-simulation.md","sourceDirName":"docs/digital-twin","slug":"/docs/digital-twin/sensor-simulation","permalink":"/docs/docs/digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docs/digital-twin/sensor-simulation.md","tags":[],"version":"current","frontMatter":{"title":"Sensor Simulation for Humanoids","sidebar_label":"Sensor Simulation","description":"Simulating various sensors for humanoid robots in digital twin environments","keywords":["Sensor Simulation","Digital Twin","Humanoid Robots","LiDAR","Depth Cameras","IMU","ROS 2"]},"sidebar":"tutorialSidebar","previous":{"title":"Unity Environments","permalink":"/docs/docs/digital-twin/unity-environments"},"next":{"title":"Tutorial Intro","permalink":"/docs/intro"}}');var a=i(4848),o=i(8453);const r={title:"Sensor Simulation for Humanoids",sidebar_label:"Sensor Simulation",description:"Simulating various sensors for humanoid robots in digital twin environments",keywords:["Sensor Simulation","Digital Twin","Humanoid Robots","LiDAR","Depth Cameras","IMU","ROS 2"]},t="Sensor Simulation for Humanoids",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Types of Sensors in Humanoid Robots",id:"types-of-sensors-in-humanoid-robots",level:2},{value:"LiDAR Sensors",id:"lidar-sensors",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:3},{value:"Simulating LiDAR Sensors",id:"simulating-lidar-sensors",level:2},{value:"2D LiDAR Simulation",id:"2d-lidar-simulation",level:3},{value:"3D LiDAR Simulation",id:"3d-lidar-simulation",level:3},{value:"Simulating Depth Cameras",id:"simulating-depth-cameras",level:2},{value:"RGB-D Camera Simulation",id:"rgb-d-camera-simulation",level:3},{value:"Sensor Noise and Calibration",id:"sensor-noise-and-calibration",level:3},{value:"Simulating IMU Sensors",id:"simulating-imu-sensors",level:2},{value:"IMU Configuration in Gazebo",id:"imu-configuration-in-gazebo",level:3},{value:"Sensor Data Flow into ROS 2",id:"sensor-data-flow-into-ros-2",level:2},{value:"Topic Architecture",id:"topic-architecture",level:3},{value:"Sensor Data Processing Pipeline",id:"sensor-data-processing-pipeline",level:3},{value:"Using Simulated Sensors for Perception Testing",id:"using-simulated-sensors-for-perception-testing",level:2},{value:"Perception Algorithm Development",id:"perception-algorithm-development",level:3},{value:"Object Detection Testing",id:"object-detection-testing",level:4},{value:"SLAM Testing",id:"slam-testing",level:4},{value:"Human Detection",id:"human-detection",level:4},{value:"Sensor Fusion Validation",id:"sensor-fusion-validation",level:3},{value:"Practical Example: Humanoid Robot Sensor Suite",id:"practical-example-humanoid-robot-sensor-suite",level:2},{value:"Quality Assurance for Sensor Simulation",id:"quality-assurance-for-sensor-simulation",level:2},{value:"Validation Techniques",id:"validation-techniques",level:3},{value:"Performance Considerations",id:"performance-considerations",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"sensor-simulation-for-humanoids",children:"Sensor Simulation for Humanoids"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a critical component of digital twin environments for humanoid robots, enabling the testing of perception algorithms and sensor fusion techniques without the risks and costs associated with real hardware. This chapter explores how to simulate various sensors commonly found on humanoid robots and how to integrate their data into ROS 2 for comprehensive testing."}),"\n",(0,a.jsx)(e.h2,{id:"types-of-sensors-in-humanoid-robots",children:"Types of Sensors in Humanoid Robots"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots typically employ multiple sensor types to perceive their environment:"}),"\n",(0,a.jsx)(e.h3,{id:"lidar-sensors",children:"LiDAR Sensors"}),"\n",(0,a.jsx)(e.p,{children:"LiDAR (Light Detection and Ranging) sensors are essential for humanoid robots due to their ability to create accurate 2D and 3D maps of the environment."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Characteristics:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Range"}),": Typically 0.1m to 30m effective range"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Accuracy"}),": Millimeter-level distance measurements"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Field of View"}),": 180\xb0 to 360\xb0 horizontal coverage"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Update Rate"}),": 5-20 Hz for typical robotics applications"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Format"}),": LaserScan or PointCloud2 messages"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Applications in Humanoid Robots:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Navigation and obstacle avoidance"}),"\n",(0,a.jsx)(e.li,{children:"Environment mapping and localization"}),"\n",(0,a.jsx)(e.li,{children:"Path planning and collision detection"}),"\n",(0,a.jsx)(e.li,{children:"Human detection and tracking"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras provide 3D spatial information, crucial for humanoid robots that need to interact with objects and navigate complex environments."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Characteristics:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Resolution"}),": 320\xd7240 to 640\xd7480 pixels"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Depth Range"}),": 0.3m to 5m effective range"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Accuracy"}),": Millimeter-level depth precision"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Update Rate"}),": 15-30 Hz"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Format"}),": Image messages with depth information"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Applications in Humanoid Robots:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Object recognition and manipulation"}),"\n",(0,a.jsx)(e.li,{children:"3D scene understanding"}),"\n",(0,a.jsx)(e.li,{children:"Human pose estimation"}),"\n",(0,a.jsx)(e.li,{children:"Surface normal estimation"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,a.jsx)(e.p,{children:"IMUs are critical for humanoid robot stability and balance, providing data about the robot's orientation and motion."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Characteristics:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensors"}),": 3-axis accelerometer, 3-axis gyroscope, 3-axis magnetometer"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Update Rate"}),": 100-1000 Hz for high-performance IMUs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Accuracy"}),": Depends on sensor quality and calibration"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Format"}),": Imu message type in ROS 2"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Applications in Humanoid Robots:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Balance and posture control"}),"\n",(0,a.jsx)(e.li,{children:"Motion tracking and gait analysis"}),"\n",(0,a.jsx)(e.li,{children:"Orientation estimation"}),"\n",(0,a.jsx)(e.li,{children:"Fall detection and recovery"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"simulating-lidar-sensors",children:"Simulating LiDAR Sensors"}),"\n",(0,a.jsx)(e.h3,{id:"2d-lidar-simulation",children:"2D LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Creating a 2D LiDAR sensor in Gazebo involves defining a ray sensor in the URDF model:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<link name="lidar_link">\n  <visual>\n    <geometry>\n      <cylinder radius="0.05" length="0.04"/>\n    </geometry>\n    <material name="black"/>\n  </visual>\n  <collision>\n    <geometry>\n      <cylinder radius="0.05" length="0.04"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.1"/>\n    <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>\n  </inertial>\n</link>\n\n<joint name="lidar_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="lidar_link"/>\n  <origin xyz="0 0 1.0" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="lidar_link">\n  <sensor type="ray" name="lidar_sensor">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/robot1</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"3d-lidar-simulation",children:"3D LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"For more advanced humanoid applications, 3D LiDAR simulation provides comprehensive spatial understanding:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="lidar_link">\n  <sensor type="ray" name="velodyne_sensor">\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.436</min_angle>\n          <max_angle>0.436</max_angle>\n        </vertical>\n      </scan>\n    </ray>\n    <plugin name="velodyne_controller" filename="libgazebo_ros_velodyne_gpu_laser.so">\n      <topic_name>points</topic_name>\n      <frame_name>lidar_link</frame_name>\n      <min_range>0.9</min_range>\n      <max_range>100.0</max_range>\n      <gaussian_noise>0.008</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"simulating-depth-cameras",children:"Simulating Depth Cameras"}),"\n",(0,a.jsx)(e.h3,{id:"rgb-d-camera-simulation",children:"RGB-D Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras are typically implemented as stereo camera sensors or structured light sensors in simulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<link name="camera_link">\n  <visual>\n    <geometry>\n      <box size="0.02 0.08 0.04"/>\n    </geometry>\n    <material name="black"/>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.02 0.08 0.04"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.05"/>\n    <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>\n  </inertial>\n</link>\n\n<joint name="camera_joint" type="fixed">\n  <parent link="head_link"/>\n  <child link="camera_link"/>\n  <origin xyz="0.05 0 0.05" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="camera_link">\n  <sensor type="depth" name="camera_sensor">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <format>R8G8B8</format>\n        <width>640</width>\n        <height>480</height>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>camera</cameraName>\n      <imageTopicName>rgb/image_raw</imageTopicName>\n      <depthImageTopicName>depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>camera_link</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"sensor-noise-and-calibration",children:"Sensor Noise and Calibration"}),"\n",(0,a.jsx)(e.p,{children:"Realistic sensor simulation includes noise models that reflect actual sensor characteristics:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n  \x3c!-- ... other parameters ... --\x3e\n  <image>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>\n    </noise>\n  </image>\n  <depth_camera>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </depth_camera>\n</plugin>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"simulating-imu-sensors",children:"Simulating IMU Sensors"}),"\n",(0,a.jsx)(e.h3,{id:"imu-configuration-in-gazebo",children:"IMU Configuration in Gazebo"}),"\n",(0,a.jsx)(e.p,{children:"IMU sensors are crucial for humanoid balance and orientation estimation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<link name="imu_link">\n  <inertial>\n    <mass value="0.01"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n\n<joint name="imu_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="imu_link"/>\n  <origin xyz="0 0 0" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="imu_link">\n  <gravity>true</gravity>\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>true</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/robot1</namespace>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n      <body_name>imu_link</body_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-data-flow-into-ros-2",children:"Sensor Data Flow into ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"topic-architecture",children:"Topic Architecture"}),"\n",(0,a.jsx)(e.p,{children:"The simulated sensors publish data to ROS 2 topics following standard message formats:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# LiDAR data\n/robot1/scan -> sensor_msgs/LaserScan\n\n# Depth camera data\n/robot1/camera/rgb/image_raw -> sensor_msgs/Image\n/robot1/camera/depth/image_raw -> sensor_msgs/Image\n/robot1/camera/depth/points -> sensor_msgs/PointCloud2\n\n# IMU data\n/robot1/imu/data -> sensor_msgs/Imu\n"})}),"\n",(0,a.jsx)(e.h3,{id:"sensor-data-processing-pipeline",children:"Sensor Data Processing Pipeline"}),"\n",(0,a.jsx)(e.p,{children:"A typical sensor data processing pipeline might look like this:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion_node\')\n\n        # Create subscribers for all sensor types\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/robot1/scan\', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, \'/robot1/imu/data\', self.imu_callback, 10)\n        self.image_sub = self.create_subscription(\n            Image, \'/robot1/camera/rgb/image_raw\', self.image_callback, 10)\n\n        # Create publisher for fused data\n        self.fused_pub = self.create_publisher(\n            # Custom fused sensor message, if needed\n            \'sensor_msgs/CombinedSensorData\', \'/robot1/sensor_fusion\', 10)\n\n        self.bridge = CvBridge()\n\n    def scan_callback(self, msg):\n        """Process LiDAR scan data"""\n        # Process laser scan for obstacle detection\n        ranges = np.array(msg.ranges)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Perform obstacle detection algorithms\n        obstacles = self.detect_obstacles(valid_ranges)\n\n        # Store processed data for fusion\n        self.last_scan = obstacles\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        # Extract orientation and acceleration\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n\n        # Process orientation for balance control\n        euler_orientation = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w)\n\n        # Store processed data for fusion\n        self.last_orientation = euler_orientation\n\n    def image_callback(self, msg):\n        """Process camera image data"""\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n        # Perform image processing\n        processed_image = self.process_image(cv_image)\n\n        # Store processed data for fusion\n        self.last_image = processed_image\n\n    def detect_obstacles(self, ranges):\n        """Simple obstacle detection from laser ranges"""\n        # Implement obstacle detection algorithm\n        # This is a simplified example\n        min_distance = np.min(ranges)\n        obstacle_angles = np.where(ranges < 1.0)[0]  # Objects within 1m\n\n        return {\n            \'min_distance\': min_distance,\n            \'obstacle_angles\': obstacle_angles,\n            \'obstacle_count\': len(obstacle_angles)\n        }\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles"""\n        # Simplified conversion\n        import math\n\n        t0 = +2.0 * (w * x + y * z)\n        t1 = +1.0 - 2.0 * (x * x + y * y)\n        roll = math.atan2(t0, t1)\n\n        t2 = +2.0 * (w * y - z * x)\n        t2 = +1.0 if t2 > +1.0 else t2\n        t2 = -1.0 if t2 < -1.0 else t2\n        pitch = math.asin(t2)\n\n        t3 = +2.0 * (w * z + x * y)\n        t4 = +1.0 - 2.0 * (y * y + z * z)\n        yaw = math.atan2(t3, t4)\n\n        return (roll, pitch, yaw)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(sensor_fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"using-simulated-sensors-for-perception-testing",children:"Using Simulated Sensors for Perception Testing"}),"\n",(0,a.jsx)(e.h3,{id:"perception-algorithm-development",children:"Perception Algorithm Development"}),"\n",(0,a.jsx)(e.p,{children:"The simulated sensors enable comprehensive testing of perception algorithms:"}),"\n",(0,a.jsx)(e.h4,{id:"object-detection-testing",children:"Object Detection Testing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Test object detection with synthetic data\nros2 run perception_package object_detector \\\n  --ros-args \\\n  -p image_topic:=/robot1/camera/rgb/image_raw \\\n  -p confidence_threshold:=0.7 \\\n  -p model_path:=/path/to/model.onnx\n"})}),"\n",(0,a.jsx)(e.h4,{id:"slam-testing",children:"SLAM Testing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Test SLAM with simulated sensors\nros2 launch slam_toolbox mapping.launch.py \\\n  use_sim_time:=true \\\n  params_file:=/path/to/slam_params.yaml\n"})}),"\n",(0,a.jsx)(e.h4,{id:"human-detection",children:"Human Detection"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Test human detection in simulation\nros2 run human_detection human_detector \\\n  --ros-args \\\n  -p camera_topic:=/robot1/camera/rgb/image_raw \\\n  -p lidar_topic:=/robot1/scan\n"})}),"\n",(0,a.jsx)(e.h3,{id:"sensor-fusion-validation",children:"Sensor Fusion Validation"}),"\n",(0,a.jsx)(e.p,{children:"Testing sensor fusion algorithms in simulation allows for:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cross-Validation"}),": Comparing different sensor outputs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Ground Truth"}),": Access to true robot states for algorithm validation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Controlled Scenarios"}),": Testing specific situations repeatedly"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Performance Metrics"}),": Quantitative evaluation of sensor performance"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practical-example-humanoid-robot-sensor-suite",children:"Practical Example: Humanoid Robot Sensor Suite"}),"\n",(0,a.jsx)(e.p,{children:"A complete sensor configuration for a humanoid robot might include:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Head-mounted sensors --\x3e\n<gazebo reference="head_link">\n  \x3c!-- RGB-D camera for perception --\x3e\n  <sensor type="depth" name="head_camera">\n    \x3c!-- Camera configuration --\x3e\n  </sensor>\n\n  \x3c!-- Infrared sensor for close-range detection --\x3e\n  <sensor type="ray" name="head_ir">\n    \x3c!-- IR sensor configuration --\x3e\n  </sensor>\n</gazebo>\n\n\x3c!-- Torso-mounted sensors --\x3e\n<gazebo reference="torso_link">\n  \x3c!-- IMU for balance --\x3e\n  <sensor type="imu" name="torso_imu">\n    \x3c!-- IMU configuration --\x3e\n  </sensor>\n\n  \x3c!-- Ultrasonic sensors for obstacle detection --\x3e\n  <sensor type="ray" name="torso_sonar_left">\n    \x3c!-- Sonar configuration --\x3e\n  </sensor>\n</gazebo>\n\n\x3c!-- Leg-mounted sensors --\x3e\n<gazebo reference="l_foot_link">\n  \x3c!-- Force/Torque sensors for balance --\x3e\n  <sensor type="force_torque" name="left_foot_ft">\n    \x3c!-- FT sensor configuration --\x3e\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"quality-assurance-for-sensor-simulation",children:"Quality Assurance for Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"validation-techniques",children:"Validation Techniques"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cross-Validation"}),": Compare simulated sensor outputs with real sensor data"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Ground Truth Verification"}),": Ensure simulated measurements match known environment properties"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Timing Analysis"}),": Verify sensor update rates and synchronization"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Noise Characterization"}),": Validate that simulated noise matches real sensor characteristics"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simulation Speed"}),": Ensure sensor simulation doesn't slow down physics simulation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Resource Usage"}),": Monitor CPU and GPU usage for complex sensor simulations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Factor"}),": Maintain real-time simulation performance with multiple sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Throughput"}),": Ensure sensor data can be processed at required rates"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is fundamental to the digital twin concept for humanoid robots, enabling comprehensive testing of perception, navigation, and interaction algorithms. By accurately simulating LiDAR, depth cameras, and IMUs, researchers can develop and validate robotic systems in a safe, controlled, and cost-effective environment before deploying to real hardware."}),"\n",(0,a.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Sensor simulation enables safe and repeatable testing of robotic algorithms"}),"\n",(0,a.jsx)(e.li,{children:"LiDAR, depth cameras, and IMUs each serve distinct functions in humanoid robots"}),"\n",(0,a.jsx)(e.li,{children:"Realistic noise models are crucial for accurate simulation"}),"\n",(0,a.jsx)(e.li,{children:"Sensor data flows into ROS 2 using standard message formats"}),"\n",(0,a.jsx)(e.li,{children:"Perception testing in simulation accelerates development and validation"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>t});var s=i(6540);const a={},o=s.createContext(a);function r(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);